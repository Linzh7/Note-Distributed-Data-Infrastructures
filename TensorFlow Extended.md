The engineering side of machine learning used to write custom codes to achieve goal, which leads to a lower code quality and redundant work and some potential vulnerabilities or hazards as results. Therefore, the people working at Google developed a platform that integrates components of a machine learning system. However, the questions that should be solved are not limited to code quality, the differences in environments between training and production could also cause many kinds of failures. Additionally, the commercial products also require lots of features, like stream training and automated pipeline construction. Another important factor is the volume of models.

Nowadays, the convolutional neural networks tend to be too big to train on a single computer. The good news is that the management of distributed computing is solved at TensorFlow [1]. TFX uses TensorFlow as its framework and has inherited the flexibility of TensorFlow, which means users can switch models without reworking the whole project. As previously stated, stream training allows the model to fit not only the existing dataset but also new data. Moreover, as a commercial product, TFX also pays attention to simplicity. Therefore, they provide a uniform interface, error handling, and resource management procedures. To my surprise, TFX also offers model validation to ensure the model is fine-tuned and well-trained.

Comparing with [[ADLS]], which was published in the same year, we can easily find that TFX considers more aspects than [[ADLS]]. For example, TFX considers what happens if the model is over-fitting and how we can avoid deploying a "bad" model to production. Besides, while Microsoft is building their data analysis system, Google has published a paper on its machine learning platform. On the other hand, there are also some similarities between them. Both of them provide interfaces for users in order to make the whole system transparent and let them focus on their tasks. Moreover, transfer learning is the inspiration for the concept of warm-starting, [[Giraph++]] is also inspired by vertex-centered models. Both of the pairs are for the optimization and development of the prototype. Specifically, for a task like training machine learning models, if we deploy model-parallel or data-parallel approaches in [[Spark]], it may not work as well as we expect. The reason is that the model parameters need to be updated frequently, which leads to additional overhead. But the RDD may perform better in model inference if the model is larger rather than deeper. MapReduce, on the other hand, makes it difficult to provide more valuable analysis on this topic because of the slow parameter updates. Additionally, it is worth mentioning that graphical convolutional neural networks (and spectral convolutional neural networks) may perform better with Giraph++ because the diffusion-based graphical convolutional neural network can be well optimized on [[Gieaph]]++. Moreover, TFX benefits from the static graph feature of TensorFlow [1], the training and inferring speeds are quicker. However, the above systems we discussed cannot provide a uniform interface, and debugging is not friendly enough, not to mention providing additional checking before pushing to production. Hence, I would consider TFX a successful commercial product for machine learning pipeline projects.


1. [TensorFlow: A System for Large-Scale Machine Learning](https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi)
2. [GRAPH CONVOLUTIONAL NETWORKS](https://tkipf.github.io/graph-convolutional-networks/)