[The Hadoop Distributed File System](https://ieeexplore.ieee.org/abstract/document/5496972)

Hadoop is a distributed framework developed by Apache, and the core of Hadoop is the storage, namely, the Hadoop Distributed File System (HDFS).

HDFS and other distributed file systems use the same strategies to organize storage space. They mark the manager computers as *NameNode* and the storage computers as *DataNode*. The metadata stored at NameNode are the records of permission, date, namespace, etc., which are maintained in memory to increase the access speed. Also, these metadata have backups and logs to improve fault tolerance. Additionally, NameNodes could also play another role in HDFS: *CheckpointNode* or *BackupNode*. CheckpointNode usually downloads the recent checkpoint and logs, then combines them to get a new checkpoint and returns it to NameNode. BackupNode is more like a shadow of NameNode, it will both create checkpoints and maintain a real-time image.

The biggest difference between HDFS and other systems is that HDFS copies the same data to several DataNodes instead of protecting it with RAID. That is a trade-off between data availability and available storage space. This strategy enables users to access data from the closest DataNode. HDFS stores data in three different DataNodes by default. It is also worth mentioning that the location of three DataNodes. The first DataNodes and second one are located at the same rack, while the third DataNode is located at a different rack. This strategy is aimed to improving write performance and saving bandwidth without harming fault tolerance. Specifically, if one rack faces some problems, there are still other copies available. The two copies on the same rack could reduce the traffic between racks, because they only use ToR switch.

![[Pasted image 20221108175156.png]]

According to Brewerâ€™s CAP theorem, the I/O operation is usually the trickiest part in distributed systems, as those systems should always maintain partition tolerance. The result of the tread-off is that HDFS gives up a certain level of availability because the authors acknowledge that programmer cannot be certain of the time to process an operation. Conversely, we can see that did well in consistency and partition tolerance by lease and verifying data with NameNodes. From this point, HDFS can handle storage of immutable documents easily, like the cache of search engines, some released statistics files, etc. Those data are viewed frequently and will not be changed. However, HDFS is not suitable for files that often need to be modified, e.g., databases, because it is difficult to keep all copies consistent. Moreover, we can imagine that if DataNodes located in different countries, users could access the data nearest to them, thus speeding up access and achieving load balance. HDFS supports MapReduce and Spark well, and Spark could be deployed at different levels in Hadoop. According to the principle that calculation could get close to data, computing nodes always use the closest copy. Furthermore, the outcome will be saved in HDFS. HDFS not only saves bandwidth and accelerates calculations but also guarantees the security of the results. Briefly, HDFS is suitable for read- and write-based applications with little modification or update, and most incremental applications can be deployed very well.


