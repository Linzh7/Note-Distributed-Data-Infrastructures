
[Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing](https://dl.acm.org/doi/10.5555/2228298.2228301)
[PDF Link](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)

To address the performance problem in [[MapReduce]] and [[Dryad]] caused by disk operation and the lack of distributed memory abstraction, an in-memory computing method was proposed named *Resilient Distributed Datasets* (RDDs). 

RDDs ensure the consistency by the read-only attributes, which means a RDD can only be created from data or another RDD. This feature also allows running backup copies to accelerates the entire task without conflict. Moreover, RDD also can *lazy compute*, i.e. the content will be computed only when they will be used (until `.count()`, `collect()`, etc. were called), which enable pipeline transformations. All of those features <>

Spark was an implement of RDD, it has been deployed in many areas. After deploying Spark at Google, some strange errors occur frequently. After the investigation, they found those errors are related to cheap memories which do not have erasure coding. However, the performance of the cluster is not decrease significantly. The reason is Spark have an efficient fault tolerance. To achieve that, Spark prefers to use lineage to recover the current dataset instead of log every operation, which reduce the overhead of checkpointing and also provides effective fault tolerance. That is to say, if some RDDs are lost or some nodes stuck, other nodes are able to reproduce the RDDs easily.

Compare with the previous framework, the biggest advantage is the speed in general tasks. Besides, RDD also have good operability and performance. Another advantage that cannot be ignored is the latency. Though, Spark cannot process data in near real-time like stream processing, e.g. Apache Storm, but at that time, Spark undoubtedly is an innovation. We also should emphasize that Spark is still widely used nowadays, because Spark provides the interactive data exploration. Conversely, Spark still has some disadvantages. Due to the coarse-grained transformations, those tasks which need shared records fine-grained among nodes cannot deploy on Spark. Besides, Spark cannot ensure the type safety in compiling because *Scala tuples* are used in implement, which might be a trade-off for generalization.