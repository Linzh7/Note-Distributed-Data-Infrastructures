
[Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing](https://dl.acm.org/doi/10.5555/2228298.2228301)
[PDF Link](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)

To address the performance problem in [[MapReduce]] and [[Dryad]] caused by disk operations and the lack of distributed memory abstraction, an in-memory computing method was proposed named *Resilient Distributed Datasets* (RDDs).

RDDs ensure consistency by having read-only attributes, which means an RDD can only be created from data or another RDD. This feature also allows for running backup copies to accelerate the entire task without conflict. Moreover, RDD also can *lazy compute*, i.e., the content will be computed only when it is used (until `.count()`, `collect()`, etc. are called), which enables pipeline transformations. In some situations, e.g., when computing the PageRank, recovering results instead of saving and loading the checkpoint can save lots of disk space because the volume of raw data is too large. Additionally, if memory is full, Spark could auto-write some RDDs to disk by descending priority, which programmers provide. The job scheduling is similar to [[Dryad]] with an additional memory trace. The manager will build a DAG to assign tasks. The fault-tolerance is also included by re-running tasks on another node.

Spark was an implementation of RDD, and it has been deployed in many areas. At first, Spark only supported Scala, a concise and efficient programming language. But with Python becoming the preferred programming language in data science, Spark also supports Python by PySpark library. As people would expect, PySpark also supports PySpark ML, GraphFrames, etc., which extends the scope of Spark. After deploying Spark at Google in the early 2010s, some strange errors occurred frequently. After the investigation, they found those errors are related to cheap memories, which do not have erasure coding. However, the performance of the cluster is not decreased significantly. The reason is that Spark has efficient fault tolerance. To achieve that, Spark prefers to use lineage to recover the current dataset instead of logging every operation, which reduces the overhead of checkpointing and also provides effective fault tolerance.

From this example, we can see that Spark can handle slow nodes easily, this idea might be inherited from MapReduce. Compared with the MapReduce, the biggest advantage of Spark is the speed of general tasks. Besides, RDD also has good operability and performance. Another advantage that cannot be ignored is the latency. Though Spark cannot process data in near real-time like stream processing, e.g., Apache Storm, at that time, Spark undoubtedly is an innovation, it reduces the latency from minutes to seconds. We also should emphasize that Spark is still widely used nowadays because Spark provides interactive data exploration, which is not provided by MapReduce and Storm. Conversely, Spark still has some disadvantages. Due to the coarse-grained transformations, those tasks that need shared records fine-grained among nodes cannot be deployed on Spark. However, MapReduce could do it well. Besides, Spark Scala API cannot ensure type safety in compiling because _Scala tuples_ are used to implement, which might be a trade-off for generalization. Additionally, Spark also provides an efficient data reuse solution, while MapReduce have to use another external storage. The functions in Spark and MapReduce are also similar. Both of them have map(flatMap) and reduce functions, which is also convenient for programmers to migrate the platform.


