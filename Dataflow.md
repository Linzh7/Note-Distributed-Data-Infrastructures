
From the perspective of practicality, a system cannot achieve all three parts: correctness, latency, and cost. Namely, each project must have a trade-off between them. Therefore, disparate implementers are focused on different areas, leading to different systems. Those systems, however, make problems more tricky by trying to give a specific solution. As mentioned by the authors, we should change our process approach for those problems, i.e., provide principled abstractions that allow users to make tradeoffs between correctness, latency, and cost. For a system like this, the biggest difference is that previous systems focused on the bounded data itself, which conflicts with the enormous and disordered data nowadays.

Dataflow then proposed a unified model that provides a flexible model by decomposing the pipeline, separating the logical and physical implementations, and enabling event-time computing with the ability to handle processing time. In this context, we can say that Dataflow is a convenient model for creating data processing pipelines.

Windowing can split the unbounded data into finite chunks in order to do some specific operations. Moreover, Dataflow also offers correct and repeatable results by using watermarks to establish a model to measure the skew between event time and processing time. Additionally, the system requires support for data windows as well as signals to indicate the end of windows. However, due to the instability of data arrival, global metrics cannot satisfy the requirements. They proposed triggers as a result, which provide finer control over windows.

[[MapReduce]] is a good start in the field of scale computing, and its successors have also contributed to building the unified platform, Hadoop. However, the shortcomings, like latency, are also still problems discussed a lot. Moreover, [[MapReduce]] and others did not propose a pipeline system to simplify the data processing procedure. [[Spark]] Stream and Storm also enable processing with low latency and good scalability. However, they cannot offer exactly-once semantics or limited windowing semantics. As for some specific tasks, like graph processing, Dataflow may still not work as well as the customized system for graphs. Comparing with [[Tensorflow Extended]] and [[Ray]], we can find that both of them want to offer a platform for deep learning solutions. Dataflow, however, wants to change the method for processing data instead of just providing a platform. In this way, maybe we can say that the aim of Dataflow is more likely to replace the classic data processing systems, e.g. Hadoop and Spark Stream, with a system that can handle the infinite input data, which is a common situation in recent years. In other words, Dataflow identified the shortcomings of modern data flow processing systems and proposed trade-offs for the three most important aspects: correctness, latency, and cost. Moreover, it also supports for the unaligned and event-time ordering windows, which are needed by modern data consumers. In this sense, Dataflow, like Mapreduce and TFX, proposes a new system pattern.