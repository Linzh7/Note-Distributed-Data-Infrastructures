[Link](https://www.usenix.org/conference/osdi-04/mapreduce-simplified-data-processing-large-clusters)

In the past twenty years, the volume of data on the Internet has increased rapidly. One of the results was the process of data became difficult even with the fast development of computing power. Specifically, companies which need to process data want to figure out a way to compute those data with cheap servers. Moreover, service providers need to provide users with results in a short time, in other words, companies are sensitive to delay. That might be the reason why this technique appears in Google, the largest internet service provider at that time.

As a result, to solve this problem, Google proposed a method named MapReduce, which enables the manager node to divide a huge task into many subtasks in order to distribute those subtasks to work nodes from a low-level perspective. Moreover, those functions are easy to understand and implement, which means this framework reduces the threshold for big data processing and improves the company's development efficiency. Additionally, it is worth to mention that a high-end server is rare and expensive, whereas common servers only have limited computing power to handle simple tasks. Consequently, companies want to solve problems with those cheap servers instead of high-end servers.

As the name shows, MapReduce can be divided into two parts: the Map function and the Reduce function. The intermediate link between these two parts is key-value pairs. The Map function processes the splitted data into key-value pairs, and the Reduce function aggregates the pairs by key. This simple procedure is the high-level interface of MapReduce. Besides, there are still some highlights in the design. Because MapReduce is running on clusters, it should deal with failures properly. Therefore, the designer programmed functions to solve different failures, like the manager node failure. To save bandwidth, a worker node should work on the data close to it. Additionally, the authors find that few tasks cost lots of time, and if we ignore the last few completed nodes, the time cost will reduce significantly. In this way, companies could not only make use of cheap servers but also improve the efficiency, though it is not as ideal as Spark.

After MapReduce was published, lots of organizations involved in this field. Due to the development of Hadoop, Apache became famous in this area. As mentioned at the beginning, companies tend to be the most active contributors. Facebook is a main contributor to Hadoop, Twitter also open sourced Heron. But we have to say, these ideas all rely on Google's paper, namely, that there is no doubt that Google is the top research company in this field.