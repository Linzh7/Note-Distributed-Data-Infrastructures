[Link](https://www.usenix.org/conference/osdi-04/mapreduce-simplified-data-processing-large-clusters)

In recent years, the volume of data has increased rapidly. One of the results was the process of data became difficult. Specifically, we need computers to work as a cluster to deal with the data in order to complete tasks on time. The reason why this technique appears in Google is that only those companies need to finish tasks in a short time, in other words, they are sensitive to delay. As a result, to solve this problem, Google proposed a method named MapReduce, which enables the manager node to divide a huge task into many subtasks in order to distribute those subtasks to work nodes from a low-level perspective. Moreover, those functions are easy to understand and implement, which means this framework reduces the threshold for big data processing and improves the company's development efficiency. Additionally, it is worth to mention that a high-end server is rare and expensive, whereas common servers only have limited computing power to handle simple tasks. Consequently, companies want to solve problems with those cheap servers instead of high-end servers.

As the name shows, MapReduce can be divided into two parts: the Map function and the Reduce function. The intermediate link between these two parts is key-value pairs. The Map function processes the splitted data into key-value pairs, and the Reduce function aggregates the pairs by key. This simple procedure is the high-level interface of MapReduce. Besides, there are still some highlights in the design. Because MapReduce is running on clusters, it should deal with failures properly. Therefore, the designer programmed functions to solve different failures, like the manager node failure. To save bandwidth, a worker node should work on the data close to it. Additionally, the authors find that few tasks cost lots of time, and if we ignore the last few completed nodes, the time cost will reduce significantly. In this way, companies could not only make use of cheap servers but also improve the efficiency (not as ideal as Spark).

After MapReduce was published, lots of organizations involved in this field. Due to the development of Hadoop, Apache became famous in this area. As mentioned at the beginning, companies tend to be the most active contributors. Facebook is a main contributor to Hadoop, Twitter also open sourced Heron. But we have to say, these ideas all rely on Google's paper, namely, that there is no doubt that Google is the top research company in this field.